select(sentence) %>%
mutate(sentence = str_squish(sentence)) |>
mutate(sentence = tolower(sentence)) |>
mutate(sentence = str_replace_all(sentence, "Privacy Policy|title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|Author.", "")) |>
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) |>
mutate(sentence = tolower(sentence)) |>
mutate(sentence = str_replace_all(sentence, "title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|Author.", "")) |>
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
View(bigrams)
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) |>
mutate(sentence = tolower(sentence)) |>
mutate(sentence = str_replace_all(sentence, "title|pages|publication date|publication subject|issn|language of publication: english|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|wwwalt
|Author.", "")) |>
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
View(bigrams)
# top_20_bigrams
top_20_bigrams <- bigrams %>%
slice_max(n, n = 20) %>%
mutate(bigram = paste(word1, " ", word2)) %>%
select(bigram, n)
top_20_bigrams
View(top_20_bigrams)
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) |>
mutate(sentence = tolower(sentence)) |>
mutate(sentence = str_replace_all(sentence, "title|pages|publication date|publication subject|issn|language of publication: english|publication   type|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|wwwalt
|Author.", "")) |>
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
# top_20_bigrams
top_20_bigrams <- bigrams %>%
slice_max(n, n = 20) %>%
mutate(bigram = paste(word1, " ", word2)) %>%
select(bigram, n)
top_20_bigrams
View(top_20_bigrams)
View(articles_df)
View(bigrams)
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) |>
mutate(sentence = tolower(sentence)) |>
mutate(sentence = str_replace_all(sentence, "title|pages|publication date|publication type|web publication|wwwindependentorg
|issn|language of publication: english|publication   type|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|wwwalt
|Author.", "")) |>
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token="ngrams", n=2 ) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
# top_20_bigrams
top_20_bigrams <- bigrams %>%
slice_max(n, n = 20) %>%
mutate(bigram = paste(word1, " ", word2)) %>%
select(bigram, n)
top_20_bigrams
View(top_20_bigrams)
data(stop_words)
force(stop_words)
View(bigrams)
View(top_20_bigrams)
# top_20_bigrams
top_20_bigrams <- bigrams %>%
slice_max(n, n = 20) %>%
mutate(bigram = paste(word1, " ", word2)) %>%
select(bigram, n)
top_20_bigrams
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) %>%
mutate(sentence = tolower(sentence)) %>%
mutate(sentence = str_replace_all(sentence,
"title|pages|publication date|publication type|web publication|issn|language of publication: english|publication type|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|wwwalt|Author\\.",
"")) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ", extra = "merge", fill = "right") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
View(bigrams)
# Define the terms to remove in a single pattern
remove_pattern <- paste(
"title|pages|publication date|publication subject|issn|language of publication: english|",
"document url|copyright|last updated|database|startofarticle|proquest document id|",
"classification|https|--|people|publication info|illustration|caption|[0-9.]|",
"identifier /keyword|twitter\\.|rauchway|keynes's|_ftn|enwikipediaorg|",
"wwwnytimescom|wwwoenbat|wwwpresidencyucsbedu|wwwalt|wwwthemoneyillusioncom|",
"aaa|predated", sep = ""
)
# Process bigrams
bigrams <- articles_df %>%
select(sentence) %>%
mutate(
sentence = str_squish(sentence),
sentence = tolower(sentence),
sentence = str_replace_all(sentence, remove_pattern, ""),
sentence = str_replace_all(sentence, "- ", "")
) %>%
unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
bigrams
View(bigrams)
View(top_20_bigrams)
# Define the terms to remove in a single pattern
remove_pattern <- paste(
"title|pages|publication date|publication subject|issn|language of publication: english|",
"document url|copyright|last updated|database|startofarticle|proquest document id|",
"classification|https|--|people|publication info|illustration|caption|[0-9.]|",
"identifier /keyword|twitter\\.|rauchway|keynes's|_ftn|enwikipediaorg|",
"wwwnytimescom|wwwoenbat|wwwpresidencyucsbedu|wwwalt|wwwthemoneyillusioncom|",
"aaa|predated", sep = ""
)
# Process bigrams
bigrams <- articles_df %>%
select(sentence) %>%
mutate(
sentence = str_squish(sentence),
sentence = tolower(sentence),
sentence = str_replace_all(sentence, remove_pattern, ""),
sentence = str_replace_all(sentence, "- ", "")
) %>%
unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(!is.na(word1))
bigrams
View(bigrams)
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
geom_bar(stat = "identity") +
theme(legend.position = "none") +
coord_flip() +
labs(title = "Raymond Moley",
caption = "n=33 articles. Graphic by Taufiq Ahmad 11-05-2024",
x = "Phrase",
y = "Count of terms")
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
geom_bar(stat = "identity") +
theme(legend.position = "none") +
coord_flip() +
labs(title = "Raymond Moley",
caption = "n=33 articles. Graphic by Kemi Busari 13-05-2024",
x = "Phrase",
y = "Count of terms")
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
geom_bar(stat = "identity") +
theme(legend.position = "none") +
coord_flip() +
labs(title = "Moley Articles",
caption = "n=33 articles. Graphic by Kemi Busari 13-05-2024",
x = "Phrase",
y = "Count of terms")
#Remove split_file folder in a cleanup
text <- pdf_text("moley_news.PDF")
#pdf_text reads the text from a PDF file.
writeLines(text, "~/GitHub/CompText_Jour/Week_11_Assignment.txt")
#writeLines writes this text to a text file
# Step 1: Read the entire text file into R
file_path <- "~/GitHub/CompText_Jour/Week_11_Assignment.txt"
text_data <- readLines(file_path)
# Step 2: Combine lines into one single string
text_combined <- paste(text_data, collapse = "\n")
# Step 3: Split the text by the "End of Document" phrase
documents <- strsplit(text_combined, "End of Document")[[1]]
# Step 4: Write each section to a new file
output_dir <- "~/GitHub/CompText_Jour/Week_11_Assignment/Week_11_extracted/"
for (i in seq_along(documents)) {
output_file <- file.path(output_dir, paste0("moley_extracted_", i, ".txt"))
writeLines(documents[[i]], output_file)
}
cat("Files created:", length(documents), "\n")
moley_index <- read_lines("~/GitHub/CompText_Jour/Week_11_Assignment/Week_11_extracted/moley_extracted_1.txt")
# Extract lines 6 to 176
extracted_lines <- moley_index[6:176]
# Print the extracted lines to the console
cat(extracted_lines, sep = "\n")
extracted_lines <- extracted_lines |>
as.data.frame()
# Step 1: Trim spaces and detect rows with titles and dates
cleaned_data <- extracted_lines |>
mutate(
# Trim leading and trailing spaces before detection
trimmed_line = str_trim(extracted_lines),
# Detect titles (start with a number and a period)
is_title = str_detect(trimmed_line, "^\\d+\\. "),
# Detect dates (e.g., "Aug 14, 2024")
is_date = str_detect(trimmed_line, "\\b\\w{3} \\d{1,2}, \\d{4}\\b")
)
# Step 2: Shift dates to align with corresponding titles
aligned_data <- cleaned_data |>
mutate(
date = ifelse(lead(is_date, 1), lead(trimmed_line, 1), NA_character_)  # Shift date to title's row
) |>
filter(is_title) |>
select(trimmed_line, date)  # Keep only the relevant columns
# Step 3: Rename columns for clarity
final_data <- aligned_data |>
rename(
title = trimmed_line,
date = date
)
#Step 4: Date and Publication in separate columns, and formatted
final_data <- separate(data = final_data, col = date, into = c("date2", "publication"), sep = "  ", extra = "merge", fill = "right")
#Step 5: Format date, clean headline
final_data <- final_data |>
mutate(date = as.Date(date2,format = "%b %d, %Y")) |>
mutate(title =str_remove(title, "^\\d+\\. ")) |>
subset(select = -(date2)) |>
mutate(index = row_number()) |>
select(index, date, title, publication)
write_csv(final_data, "Week_11_extracted/final_data.csv")
# List out text files that match pattern .txt, create DF
files <- list.files("~/GitHub/CompText_Jour/Week_11_Assignment/Week_11_extracted", pattern="*.txt") %>%
as.data.frame() |>
rename(filename = 1) |>
#create an index with the file name
mutate(index = str_extract(filename, "\\d+")) |>
mutate(index = as.numeric(index))
#the actual path: ~/GitHub/CompText_Jour/Week_11_Assignment
#Join the file list to the index
#load final data if you haven't already
#final_data <- read.csv("assets/final_data.csv")
final_index <- final_data |>
inner_join(files, c("index")) |>
#you need the actual hard-coded path on this line below to the text
# mutate(filepath = paste0("~/GitHub/CompText_Jour/Week_11_Assignment/", filename))
mutate(filepath = paste0("~/GitHub/CompText_Jour/Week_11_Assignment/Week_11_extracted/", filename))
head(final_index)
# Define function to loop through each text file
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
###
# Create elements needed to run function
###
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
###
# Execute function using lapply
# This loops through each row of the dataframe and append results to master file
###
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#After viewing articles_df, I see 64 lines from the index that I don't need. Cutting them
articles_df <- articles_df %>%
slice(-c(1:158)) |>
#gets rid of blank rows
filter(trimws(sentence) != "")
#write.csv(articles_df, "Week_11_extracted.csv")
data(stop_words)
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) %>%
mutate(sentence = tolower(sentence)) %>%
mutate(sentence = str_replace_all(sentence,
"title|pages|publication date|publication type|web publication|issn|language of publication: english|publication type|document url|copyright|last updated|database|startofarticle|proquest document id|classification|https|--|people|publication info|illustration|caption|[0-9.]|identifier /keyword|twitter\\|About LexisNexis|erms & Conditions|Copyright|2020 LexisNexis|Loose Ends|Power Line|All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note|https|wwwalt|Author\\.",
"")) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ", extra = "merge", fill = "right") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
# Define the terms to remove in a single pattern
# Define patterns to remove unwanted metadata
metadata_patterns <- paste(
"title|pages|publication date|publication type|web publication|issn",
"language of publication: english|document url|copyright|last updated|database",
"startofarticle|proquest document id|classification|https|--|people|publication info",
"illustration|caption|[0-9]+|identifier /keyword|erms & Conditions|Copyright",
"All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note",
"Author\\.|https|wwwalt",
sep = "|"
)
# Process bigrams
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) %>%
mutate(sentence = tolower(sentence)) %>%
mutate(sentence = str_remove_all(sentence, metadata_patterns)) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ", extra = "merge", fill = "right") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
# top_20_bigrams
top_20_bigrams <- bigrams %>%
slice_max(n, n = 20) %>%
mutate(bigram = paste(word1, " ", word2)) %>%
select(bigram, n)
top_20_bigrams
View(top_20_bigrams)
# Define the terms to remove in a single pattern
# Define patterns to remove unwanted metadata
metadata_patterns <- paste(
"title|pages|publication date|publication type|web publication|issn",
"language of publication: english|document url|copyright|last updated|database",
"startofarticle|proquest document id|classification|https|--|people|publication info",
"illustration|caption|[0-9]+|identifier /keyword|erms & Conditions|Copyright",
"All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note",
"Author\\.|https|wwwalt",
sep = "|"
)
# Process bigrams
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) %>%
mutate(sentence = tolower(sentence)) %>%
mutate(sentence = str_remove_all(sentence, metadata_patterns)) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ", extra = "merge", fill = "right") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
# Define function to loop through each text file
create_article_text <- function(row_value) {
#row_value is the single argument that is passed to the function
# Take each row of the dataframe
temp <- final_index %>%
slice(row_value)
# Store the filename for  use in constructing articles dataframe
temp_filename <- temp$filename
# Create a dataframe by reading in lines of a given textfile
# Add a filename column
articles_df_temp <- read_lines(temp$filepath) %>%
as_tibble() %>%
mutate(filename = temp_filename)
# Bind results to master articles_df
# <<- returns to global environment
articles_df <<- articles_df %>%
bind_rows(articles_df_temp)
}
# Create empty tibble to store results
articles_df <- tibble()
#running once to test
#create_article_text(2)
# Create an array of numbers to loop through, from 1 to the number of rows in our index dataframe
row_values <- 1:nrow(final_index)
#Execute function using lapply
lapply(row_values, create_article_text)
###
# Clean up articles_df and join to index dataframe
###
articles_df <- articles_df %>%
select(filename, sentence=value) %>%
inner_join(final_index)
#After viewing articles_df, I see 158 lines from the index that I don't need. Cutting them
articles_df <- articles_df %>%
slice(-c(1:158)) |>
#gets rid of blank rows
filter(trimws(sentence) != "")
#write.csv(articles_df, "Week_11_extracted.csv")
# Define the terms to remove in a single pattern
# Define patterns to remove unwanted metadata
metadata_patterns <- paste(
"title|pages|publication date|publication type|web publication|issn",
"language of publication: english|document url|copyright|last updated|database",
"startofarticle|proquest document id|classification|https|--|people|publication info",
"illustration|caption|[0-9]+|identifier /keyword|erms & Conditions|Copyright",
"All Rights Reserved|Length|Dateline|Byline|Series Title|Author Note",
"Author\\.|https|wwwalt",
sep = "|"
)
# Process bigrams
bigrams <- articles_df %>%
select(sentence) %>%
mutate(sentence = str_squish(sentence)) %>%
mutate(sentence = tolower(sentence)) %>%
mutate(sentence = str_remove_all(sentence, metadata_patterns)) %>%
mutate(sentence = str_replace_all(sentence, "- ", "")) %>%
unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ", extra = "merge", fill = "right") %>%
filter(!is.na(word1) & !is.na(word2)) %>%
filter(!word1 %in% stop_words$word, !word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE)
View(bigrams)
# top_20_bigrams
top_20_bigrams <- bigrams %>%
slice_max(n, n = 20) %>%
mutate(bigram = paste(word1, " ", word2)) %>%
select(bigram, n)
top_20_bigrams
ggplot(top_20_bigrams, aes(x = reorder(bigram, n), y = n, fill=n)) +
geom_bar(stat = "identity") +
theme(legend.position = "none") +
coord_flip() +
labs(title = "Moley Articles",
caption = "n=33 articles. Graphic by Kemi Busari 13-05-2024",
x = "Phrase",
y = "Count of terms")
View(bigrams)
View(top_20_bigrams)
View(articles_df)
View(articles_df)
install.packages("pdftools")
library(tidyverse)
library(pdftools)
#Remove split_file folder in a cleanup
text <- pdf_text("moley_news.PDF")
#Remove split_file folder in a cleanup
text <- pdf_text("moley_news.PDF")
#Remove split_file folder in a cleanup
text <- pdf_text("moley_news.PDF")
summary(cars)
plot(pressure)
income_vacancy %>%
ggplot(aes(x = community)) +
geom_point(aes(y=income_2020), color = "#64D197", stroke=5) +
geom_point(aes(y=vacant_2020*1000), stroke=5, color = "#FB8AE3") +
scale_y_continuous(
name = "2020 Income (Green)", limits=c(0, 62000), labels=scales::dollar_format(),
sec.axis = sec_axis(~./60000, name="2020 Percent Vacancies (Red)", labels = scales::percent))+
#scale_y_continuous(labels = scales::percent) +
labs(
title = "Income and Vacancies of Baltimore Neighborhoods 2020",
x = "Community",
caption = "source: BNIA Data - By: Parker Leipzig and Rob Wells"
)
setwd("~/GitHub/Kemi_repository")
setwd("~/GitHub/Kemi_repository")
knitr::opts_chunk$set(echo = TRUE)
#install.packages("pdftools")
library(pdftools)
library(tidyverse)
library(pdftools)
#install.packages("pdftools")
library(pdftools)
library(tidyverse)
library(pdftools)
#Load the data
file_path <- "~/GitHub/Kemi_repository/dele_giwa.txt"
#Load the data
file_path <- "~/GitHub/Kemi_repository/dele_giwa.txt"
#install.packages("pdftools")
library(pdftools)
library(tidyverse)
library(pdftools)
library(striprtf)
# Define the file path
file_path <- "~/GitHub/Kemi_repository/dele_giwa"
# Expand the tilde (~) to the full home directory path
full_path <- normalizePath(file_path)
# List all files in the directory
files <- list.files(full_path, full.names = TRUE)
# Print the list of files
print(files)
# Load a specific file (for example, a CSV file)
# Assuming there's a file "data.csv" in the directory
csv_file <- file.path(full_path, "data.csv")
data <- read.csv(csv_file)
